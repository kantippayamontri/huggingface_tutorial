{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "In this lesson, I want to learn about how to dealing with rate limit in hugging face\n",
    "1. I want to test the error message that I will get from hf\n",
    "\n",
    "** result try to make request untill reach the rate limit is not work because It have a lot of limit\n",
    "** next experiment -> try to simulate the rate limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# load env variable from .env file in the same directory\n",
    "load_dotenv() #use from jupyter remote kernel\n",
    "\n",
    "# get token\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "if not hf_token:\n",
    "    print(\"❌ ERROR: Token not found! Set HUGGINGFACE_TOKEN on your VPS\")\n",
    "else:\n",
    "    print(\"✅ Token found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(token=hf_token) # we don't specific model name so it use default model\n",
    "result = client.text_classification(\"I love this!!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provoke to see the error\n",
    "# this method not work because it's has a lot of rate limit per hour\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        prompt = f\"this is number {i}\"\n",
    "        response = client.text_classification(prompt)\n",
    "        print(f\"Request {i}: Success - {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Request {i}: Error - {response}\")\n",
    "    \n",
    "    # time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to make a lot of request in the same time -> still not work -> not reach the rate limit\n",
    "import concurrent.futures\n",
    "\n",
    "def make_request(i):\n",
    "    try:\n",
    "        response = client.text_classification(f\"text {i}\")\n",
    "        return f\"Request {i}: Success\"\n",
    "    except Exception as e:\n",
    "        return f\"Request {i}: Error - {e}\"\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:\n",
    "    futures = [executor.submit(make_request, i) for i in range(100)]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        print(future.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
